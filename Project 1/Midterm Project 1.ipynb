{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30cf167a",
   "metadata": {},
   "source": [
    "## Using Python to Perform Extract-Transform-Load (ETL Processing)\n",
    "Modern Data Warehousing and Analytics solutions frequently use languages like Python or Scala to extract data from numerous sources, including relational database management systems, NoSQL database systems, real-time streaming endpoints and Data Lakes.  These languages can then be used to perform many types of transformation before then loading the data into a variety of destinations including file systems and data warehouses. This data can then be consumed by data scientists or business analysts.\n",
    "\n",
    "In this lab you will recreate the **Northwind_DW** dimensional database from Lab 2; however, you'll take an entirely different approach. Instead of extracting, transforming and loading the date entirely on the database system entirely using SQL data definition language (DDL) and data manipulation language (DML) statements, here you will learn to interact with the RDBMS from a remote client running Python. You will learn to fetch data into Pandas DataFrames, perform all the necessary transformations in-memory on the client, and then push the newly transformed DataFrame back to the RDBMS using a Pandas function that will create the table and fill it with data with a single operation.\n",
    "\n",
    "### Prerequisites:\n",
    "#### Import the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f7fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#-----------------------------------\n",
    "\n",
    "import pymongo\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a5ce38e-fba0-40de-9e23-6fa78cd1661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Declare & Assign Connection Variables for the MySQL Server & Databases with which You'll be Working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c1a3519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Connection String: mongodb://localhost:27017/\n",
      "Atlas Connection String: mongodb+srv://ryp6vw:Premier#13!!!@cluster0.z5pf4tg.mongodb.net\n"
     ]
    }
   ],
   "source": [
    "host_name = \"localhost\"\n",
    "host_ip = \"127.0.0.1\"\n",
    "port = \"3306\"\n",
    "user_id = \"root\"\n",
    "pwd = \"Passw0rd123\"\n",
    "\n",
    "src_dbname = \"sakila\"\n",
    "dst_dbname = \"sakila_dw\"\n",
    "\n",
    "#----------------------------------------------\n",
    "\n",
    "src_dbname = \"sakila\"\n",
    "dst_dbname = \"sakila_dw\"\n",
    "mysql_uid = \"root\"\n",
    "mysql_pwd = \"Passw0rd123\"\n",
    "mysql_host = \"ds2002-mysql.mysql.database.azure.com\"\n",
    "\n",
    "atlas_cluster_name = \"cluster0.z5pf4tg\"\n",
    "atlas_user_name = \"ryp6vw\"\n",
    "atlas_password = \"Premier#13!!!\"\n",
    "\n",
    "conn_str = {\"local\" : f\"mongodb://localhost:27017/\",\n",
    "    \"atlas\" : f\"mongodb+srv://{atlas_user_name}:{atlas_password}@{atlas_cluster_name}.mongodb.net\"\n",
    "}\n",
    "\n",
    "src_dbname = \"sakila\"\n",
    "dst_dbname = \"sakila_dw\"\n",
    "\n",
    "print(f\"Local Connection String: {conn_str['local']}\")\n",
    "print(f\"Atlas Connection String: {conn_str['atlas']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca42abc",
   "metadata": {},
   "source": [
    "#### Define Functions for Getting Data From and Setting Data Into Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c9d366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(user_id, pwd, host_name, db_name, sql_query):\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    dframe = pd.read_sql(sql_query, connection);\n",
    "    connection.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "\n",
    "def set_dataframe(user_id, pwd, host_name, db_name, df, table_name, pk_column, db_operation):\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    \n",
    "    if db_operation == \"insert\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='replace')\n",
    "        sqlEngine.execute(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({pk_column});\")\n",
    "            \n",
    "    elif db_operation == \"update\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='append')\n",
    "    \n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e90f89-b839-49c8-a35b-ab5e76b779d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create the New Data Warehouse database, and to Use it, Switch the Connection Context.\n",
    "#Clearly, you won't get very far without having a database to work with. Here we demonstrate how we can *drop* a database if it already exists, and then *create* the new **northwind_dw2** database and *use* it as the target of all subsequent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70548734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x222083ae340>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}\"\n",
    "sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "\n",
    "sqlEngine.execute(f\"DROP DATABASE IF EXISTS `{dst_dbname}`;\")\n",
    "sqlEngine.execute(f\"CREATE DATABASE `{dst_dbname}`;\")\n",
    "sqlEngine.execute(f\"USE {dst_dbname};\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02169776",
   "metadata": {},
   "source": [
    "### 1.0. Create & Populate the Dimension Tables\n",
    "#### 1.1. Extract Data from the Source Database Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "398a6cee-764f-4a9a-9736-fe49b5817b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rental_id</th>\n",
       "      <th>rental_date</th>\n",
       "      <th>inventory_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>return_date</th>\n",
       "      <th>staff_id</th>\n",
       "      <th>last_update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2005-05-24 22:53:30</td>\n",
       "      <td>367</td>\n",
       "      <td>130</td>\n",
       "      <td>2005-05-26 22:04:30</td>\n",
       "      <td>1</td>\n",
       "      <td>2006-02-15 21:30:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2005-05-24 22:54:33</td>\n",
       "      <td>1525</td>\n",
       "      <td>459</td>\n",
       "      <td>2005-05-28 19:40:33</td>\n",
       "      <td>1</td>\n",
       "      <td>2006-02-15 21:30:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rental_id         rental_date  inventory_id  customer_id  \\\n",
       "0          1 2005-05-24 22:53:30           367          130   \n",
       "1          2 2005-05-24 22:54:33          1525          459   \n",
       "\n",
       "          return_date  staff_id         last_update  \n",
       "0 2005-05-26 22:04:30         1 2006-02-15 21:30:53  \n",
       "1 2005-05-28 19:40:33         1 2006-02-15 21:30:53  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_rental = \"SELECT * FROM sakila.rental;\"\n",
    "df_rental = get_dataframe(user_id, pwd, host_name, src_dbname, sql_rental)\n",
    "df_rental.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8efbe3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>address_id</th>\n",
       "      <th>active</th>\n",
       "      <th>create_date</th>\n",
       "      <th>last_update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>MARY</td>\n",
       "      <td>SMITH</td>\n",
       "      <td>MARY.SMITH@sakilacustomer.org</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2006-02-14 22:04:36</td>\n",
       "      <td>2006-02-15 04:57:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>PATRICIA</td>\n",
       "      <td>JOHNSON</td>\n",
       "      <td>PATRICIA.JOHNSON@sakilacustomer.org</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2006-02-14 22:04:36</td>\n",
       "      <td>2006-02-15 04:57:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  store_id first_name last_name  \\\n",
       "0            1         1       MARY     SMITH   \n",
       "1            2         1   PATRICIA   JOHNSON   \n",
       "\n",
       "                                 email  address_id  active  \\\n",
       "0        MARY.SMITH@sakilacustomer.org           5       1   \n",
       "1  PATRICIA.JOHNSON@sakilacustomer.org           6       1   \n",
       "\n",
       "          create_date         last_update  \n",
       "0 2006-02-14 22:04:36 2006-02-15 04:57:20  \n",
       "1 2006-02-14 22:04:36 2006-02-15 04:57:20  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_customer = \"SELECT * FROM sakila.customer;\"\n",
    "df_customer = get_dataframe(user_id, pwd, host_name, src_dbname, sql_customer)\n",
    "df_customer.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a278a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>staff_id</th>\n",
       "      <th>rental_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>payment_date</th>\n",
       "      <th>last_update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>2.99</td>\n",
       "      <td>2005-05-25 11:30:37</td>\n",
       "      <td>2006-02-15 22:12:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>573</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2005-05-28 10:35:23</td>\n",
       "      <td>2006-02-15 22:12:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_id  customer_id  staff_id  rental_id  amount        payment_date  \\\n",
       "0           1            1         1         76    2.99 2005-05-25 11:30:37   \n",
       "1           2            1         1        573    0.99 2005-05-28 10:35:23   \n",
       "\n",
       "          last_update  \n",
       "0 2006-02-15 22:12:30  \n",
       "1 2006-02-15 22:12:30  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_payment = \"SELECT * FROM sakila.payment;\"\n",
    "df_payment = get_dataframe(user_id, pwd, host_name, src_dbname, sql_payment)\n",
    "df_payment.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d15537b5-fd07-42f7-af6e-ec99ba920814",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_payment \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df_payment\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mC:\\Miniconda\\envs\\py38_default\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Miniconda\\envs\\py38_default\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Miniconda\\envs\\py38_default\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\Miniconda\\envs\\py38_default\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Miniconda\\envs\\py38_default\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\Miniconda\\envs\\py38_default\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "df_payment = pd.read_csv(\"\")\n",
    "df_payment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1900ab3f",
   "metadata": {},
   "source": [
    "#### 1.2. Perform Any Necessary Transformations\n",
    "Pandas DataFrames enable extensive data modification capabilities. Here we will start by simply dropping features (columns) that we don't believe provide any real value to our analytics solution. Examples include columns having a high percentage of NULL values, columns having large amounts of free-text, and columns having binary large object (BLOB) data such as images or other documents. Then, we will rename the primary key column (id) to conform with data warehouse design standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc0d546-5aa7-4d25-90e7-630d8b595ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['staff_id','last_update']\n",
    "df_rental.drop(drop_cols, axis=1, inplace=True)\n",
    "df_rental.rename(columns={\"rental_id\":\"rental_key\", \"customer_id\":\"customer_key\"}, inplace=True)\n",
    "\n",
    "df_rental.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c57ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['active','last_update','create_date']\n",
    "df_customer.drop(drop_cols, axis=1, inplace=True)\n",
    "df_customer.rename(columns={\"customer_id\":\"customer_key\"}, inplace=True)\n",
    "\n",
    "df_customer.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a618aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['last_update', 'payment_date']\n",
    "df_payment.drop(drop_cols, axis=1, inplace=True)\n",
    "df_payment.rename(columns={\"payment_id\":\"payment_key\",\"customer_id\":\"customer_key\"}, inplace=True)\n",
    "\n",
    "df_payment.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cecaeb",
   "metadata": {},
   "source": [
    "#### 1.4. Load the Transformed DataFrames into the New Data Warehouse by Creating New Tables\n",
    "Here I demonstrate how we can create an iterable data structure containing the values needed to correctly create and populate the new dimension tables. If you inspect this code listing carefully, you'll notice that it's a **list** containing a **set** *(or vector)* for each dimension table. Each **set** then contains the *table_name* we need to assign to the table, the *pandas DataFrame* we crafted to define & populate the table, and the name we need to assign to the *primary_key* column.  With this *list of sets* defined, we can then call our **set_dataframe( )** function from within a **for *loop*** to create each *dimension* table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b84457",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_operation = \"insert\"\n",
    "\n",
    "tables = [('dim_rental', df_rental, 'rental_key'),\n",
    "          ('dim_customer', df_customer, 'customer_key'),\n",
    "          ('dim_payment', df_payment, 'payment_key')\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74821a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name, dataframe, primary_key in tables:\n",
    "    set_dataframe(user_id, pwd, host_name, dst_dbname, dataframe, table_name, primary_key, db_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476dc940",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.0. Create & Populate the Fact Table\n",
    "Here we will learn two approaches to creating the *fact_orders* fact table. The first approach demonstrates that a carefully crafted SQL SELECT statement can be used to perform this task... *but what fun would that be.* Seriously though, this approach is quick and effect if you already have the query, but what if you didn't have the opportunity to view and work with the data beforehand?  What's more, you may be required to combine data from multiple sources, some of which may not be relational database management systems. Then, a simple SQL query won't do!  You would need to load the data from the various sources (e.g., database tables, CSV or JSON files, NoSQL document collections, API stream data) and then combine them into a single dataframe that you could then use to create a new database table. For this reason we'll see how we can retrieve the data, but we won't bother to use it for creating a new table... we already know how to do that using the **set_dataframe( )** function anyway.\n",
    "\n",
    "#### 2.1. First, you could simply use the SQL SELECT statement you authored in Lab 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16ff332",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_rental_orders = pd.merge(df_rental, df_customer, on='customer_key', how='left')\n",
    "df_fact_rental_orders.insert(0, \"step\", range(1, df_fact_rental_orders.shape[0]+1))\n",
    "\n",
    "df_fact_rental_orders.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba704b-3e73-4861-bfce-cb60dbe3ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_rental_orders = pd.merge(df_fact_rental_orders, df_payment, on='customer_key', how='left')\n",
    "df_fact_rental_orders.insert(0, \"step2\", range(1, df_fact_rental_orders.shape[0]+1))\n",
    "\n",
    "df_fact_rental_orders['rental_date'] = pd.to_datetime(df_fact_rental_orders['rental_date']).dt.date\n",
    "df_fact_rental_orders['return_date'] = pd.to_datetime(df_fact_rental_orders['return_date']).dt.date\n",
    "df_fact_rental_orders.return_date = df_fact_rental_orders.return_date.astype('datetime64')\n",
    "\n",
    "\n",
    "df_fact_rental_orders.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ce02d-a164-4787-a2e7-8cf308ab1a62",
   "metadata": {},
   "source": [
    "##### 2.2.5. Get the Data from the Date Dimension Table.\n",
    "First, fetch the Surrogate Primary Key (date_key) and the Business Key (full_date) from the Date Dimension table using the **get_dataframe()** function. Also, be certain to cast the **full_date** column to the **datetime64** data type using the **.astype()** function that is native to Pandas DataFrame columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b3c04b-b4a1-4a7a-b83a-e458a624bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dim_date = \"SELECT date_key, full_date FROM sakila.dim_date;\"\n",
    "df_dim_date = get_dataframe(user_id, pwd, host_name, src_dbname, sql_dim_date)\n",
    "df_dim_date.full_date = df_dim_date.full_date.astype('datetime64')\n",
    "df_dim_date.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a4e5a-3f03-4eff-a9cd-644bf38de18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_order_date = df_dim_date.rename(columns={\"date_key\" : \"rental_date_key\", \"full_date\" : \"return_date\"})\n",
    "df_fact_rental_orders = pd.merge(df_fact_rental_orders, df_dim_order_date, on='return_date', how='left')\n",
    "df_fact_rental_orders.drop(['rental_date', 'return_date'], axis=1, inplace=True)\n",
    "df_fact_rental_orders.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe7eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['step2', 'step', 'inventory_id', 'store_id', 'address_id', 'staff_id', 'rental_id']\n",
    "df_fact_rental_orders.drop(drop_columns, axis=1, inplace=True)\n",
    "\n",
    "# Rename Foreign Key Columns\n",
    "df_fact_rental_orders.rename(columns={\"first_name\" : \"customer_first_name\",\n",
    "                               \"last_name\" : \"customer_last_name\",\n",
    "                               \"email\" : \"customer_email\",\n",
    "                               \"amount\" : \"paid_amount\"}, inplace=True)\n",
    "\n",
    "\n",
    "# Reorder the columns\n",
    "ordered_columns = [\"customer_key\",\n",
    "                  \"payment_key\",\n",
    "                  \"rental_key\",\n",
    "                  \"rental_date_key\",\n",
    "                  \"customer_first_name\",\n",
    "                  \"customer_last_name\",\n",
    "                  \"customer_email\",\n",
    "                  \"paid_amount\"\n",
    "                  ]\n",
    "df_fact_rental_orders = df_fact_rental_orders[ordered_columns]\n",
    "df_fact_rental_orders.insert(0, \"fact_rental_order_key\", range(1, df_fact_rental_orders.shape[0]+1))\n",
    "df_fact_rental_orders.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16856516-e961-49a7-8cf7-e14869c316c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"fact_rental_orders\"\n",
    "primary_key = \"fact_rental_order_key\"\n",
    "db_operation = \"insert\"\n",
    "\n",
    "set_dataframe(user_id, pwd, host_name, dst_dbname, df_fact_rental_orders, table_name, primary_key, db_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d96a7-c5d4-4143-a68d-96d9bb67f14e",
   "metadata": {},
   "source": [
    "##### 2.2.6. Perform a Test to verify that the new DW exists and contains the correct data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ae207-f4c2-4777-874f-e2e45c7baea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_test = \"\"\"\n",
    "    SELECT rental.`customer_email` AS `customer_email`,\n",
    "        COUNT(payment.`payment_key`) AS `num_payments`,\n",
    "        SUM(payment.`paid_amount`) AS `total_paid_amount`\n",
    "    FROM `{0}`.`fact_rental_orders` AS payment\n",
    "    INNER JOIN `{0}`.`fact_rental_orders` AS rental\n",
    "    ON payment.customer_key = rental.customer_key\n",
    "    WHERE rental.`customer_email` = '{1}'\n",
    "    GROUP BY rental.`customer_email`\n",
    "\"\"\".format(dst_dbname, customer_email)\n",
    "\n",
    "df_test = get_dataframe(user_id, pwd, host_name, src_dbname, sql_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff64fc-e944-4231-81a0-12b1041f688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0be98d-3795-484e-8b4e-4aa495dd185b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
